{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NilupulNakandala/GrooveOn-Hairstyle-recommendation-/blob/main/GROOVEON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Project#"
      ],
      "metadata": {
        "id": "7dti3OIf-Gf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJVv1TuElGFm",
        "outputId": "d3030298-50f7-47d8-f247-2bdf6694e26d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # For data manipulation\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5OFWPH6nzoYA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign the dataset path to a variable\n",
        "data_directory = '/content/drive/MyDrive/SDGP_Project/Dataset'\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#face shape catogaries\n",
        "categories = os.listdir(data_directory)\n",
        "\n",
        "# references - https://towardsdatascience.com/implementing-k-nearest-neighbors-with-scikit-learn-9e4858e231ea\n",
        "\n",
        "# Initialize lists to store images and labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through each category of images\n",
        "for category in categories:\n",
        "    category_path = os.path.join(data_directory, category)\n",
        "    for image_path in os.listdir(category_path):\n",
        "        image = cv2.imread(os.path.join(category_path, image_path)) # Read the image using OpenCV\n",
        "        # Resize the image to a consistent size\n",
        "        image = cv2.resize(image, (224, 224))  # Adjust size as needed\n",
        "\n",
        "        # Append the image and its corresponding label to the lists\n",
        "        images.append(image)\n",
        "        labels.append(category)\n",
        "\n",
        "# Convert images to NumPy arrays\n",
        "images = np.array(images)\n",
        "\n",
        "#print(images)\n",
        "#print(labels)\n",
        "\n",
        "# split the dataset to Training and testing\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(images, labels, test_size = 0.2) #80% for training and 20% for testing\n",
        "\n",
        "# Assuming X_train is a list of 3D images (height, width, channels)\n",
        "X_train_flattened = []\n",
        "for image in X_train:\n",
        "    flattened_image = image.flatten()  # Reshape to 1D array\n",
        "    X_train_flattened.append(flattened_image)\n",
        "\n",
        "#Creating a Support Vector Machine (SVM) model to regression\n",
        "model = SVC() # SVM expects only 2D inputs\n",
        "model.fit(X_train_flattened, Y_train) #\n",
        "\n",
        "\n",
        "# Save the trained model for future use\n",
        "#model.save(\"face_shape_model.pkl\")"
      ],
      "metadata": {
        "id": "c-j2zFgTh9CU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "89a5b015-6c0e-4fce-f6ef-5487588ea646"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC()"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sklearn library to evaluate model performance\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# Flatten the testing images as well\n",
        "X_test_flattened = []\n",
        "for image in X_test:\n",
        "    flattened_image = image.flatten()\n",
        "    X_test_flattened.append(flattened_image)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "predicted_labels = model.predict(X_test_flattened)\n",
        "\n",
        "# Calculate accuracy, precision, recall, and confusion matrix\n",
        "accuracy = accuracy_score(Y_test, predicted_labels)\n",
        "precision = precision_score(Y_test, predicted_labels, average='weighted')\n",
        "recall = recall_score(Y_test, predicted_labels, average='weighted')\n",
        "confusion_matrix = confusion_matrix(Y_test, predicted_labels)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlHRqFnXoClT",
        "outputId": "8dec49bc-4f95-4ca5-ecba-7d272fbae8f6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "Confusion Matrix:\n",
            " [[0 0 1 2 0 0 0]\n",
            " [0 0 3 0 0 0 0]\n",
            " [0 0 0 2 0 0 2]\n",
            " [0 0 1 0 0 0 1]\n",
            " [0 0 1 0 0 1 0]\n",
            " [0 0 0 4 0 0 0]\n",
            " [0 0 1 1 0 0 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#shape prediction\n",
        "new_image = cv2.imread(\"/content/drive/MyDrive/SDGP_Project/damnd.jpeg\")\n",
        "new_image = cv2.resize(new_image, (224, 224))  # Resize to match training data\n",
        "new_features = new_image.flatten()\n",
        "predicted_label = model.predict([new_features])[0]  # Reshape as a single-sample array\n",
        "print(\"Predicted face shape:\", predicted_label)\n",
        "#print(image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CvgwoXDonFE",
        "outputId": "ebdd3771-21cd-4d02-dcc7-85c04bf0480a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted face shape: oval\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testdatadir = \"/content/NewInputData/diamond.jpeg\" # test run\n",
        "testdata = cv2.imread(testdatadir)\n",
        "print(testdata)"
      ],
      "metadata": {
        "id": "7ZQRlLwgj588"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#different approach\n",
        "\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "train = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "train_dataset = train.flow_from_directory(data_directory,target_size=(200,200),batch_size=3, class_mode = 'binary')\n",
        "\n",
        "\n",
        "\n",
        "train_dataset.class_indices\n",
        "\n",
        "classess = ['diamond','heart','oblong','oval','round','square','triangle']\n",
        "\n",
        "cnn=tf.keras.Sequential()\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=64,padding='same',strides=2,kernel_size=3,activation='relu',input_shape=(200,200,3)))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32,padding='same',strides=2,kernel_size=3,activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))\n",
        "\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32,padding='same',strides=2,kernel_size=3,activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2))\n",
        "\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "cnn.add(tf.keras.layers.Dense(7,activation='softmax'))\n",
        "\n",
        "\n",
        "cnn.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "cnn.fit(train_dataset,epochs=10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "img = cv2.imread(\"../input/face-shape-classification/face shape detector/diamond/download (10).jpg\")\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "img = cv2.resize(img,(200,200))\n",
        "img = np.reshape(img,[1,200,200,3])\n",
        "classes = cnn.predict(img)\n",
        "print(np.argmax(classes))"
      ],
      "metadata": {
        "id": "x3xGo4iMQRyR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}